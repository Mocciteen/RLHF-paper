### RLHF paper

---

#### RLHF

**综述**

**A Survey of Reinforcement Learning from Human Feedback** 202404 [paper](https://arxiv.org/pdf/2312.14925)

最先提出这一概念的相关文章

**Deep Reinforcement Learning from Human Preferences**

**Fine-Tuning Language Models from Human Preferences**

**Learning to summarize from human feedback**

**Training language models to follow instructions with human feedback**

**Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback**

---

**理论：**

**Is RLHF More Difficult than Standard RL? A Theoretical Perspective** NeurIPS2023 [paper](https://arxiv.org/pdf/2306.14111)

**Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization**

**Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint**

**RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs** 202404 [paper](https://arxiv.org/pdf/2404.08555)

**Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization** 

---

RLHF基础上进行相关改进

**Confronting Reward Model Overoptimization with Constrained RLHF**

**Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF**

**Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF**

**RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback**

**Balancing Individual Preferences and Shared Objectives in Multiagent Reinforcement Learning**

**MaxMin-RLHF: Alignment with Diverse Human Preference**

**Mapping Social Choice Theory to RLHF** ICLR2024 [paper](https://arxiv.org/pdf/2404.13038)

**PERSONALIZED SOUPS: PERSONALIZED LARGE LANGUAGE MODEL ALIGNMENT VIA POST-HOC PARAMETER MERGING**

**RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs** [paper](https://arxiv.org/pdf/2407.02552)

**Reinforcement Learning from Human Feedback with Active Queries**

---

一种高效且经典的改进方法:**DPO**

绕开奖励函数的设置直接优化，降低复杂度

**Direct Preference Optimization: Your Language Model is Secretly a Reward Model**

**Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences**

**Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation**

**Generalized Preference Optimization: A Unified Approach to Offline Alignment**

**Provably Robust DPO: Aligning Language Models with Noisy Feedback**

**Token-level Direct Preference Optimization**

---

#### human preference

采取其他方法实现对人类偏好的学习

**Understanding the Learning Dynamics of Alignment with Human Feedback**

**Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback**

**Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback**

**Human Alignment of Large Language Models through Online Preference Optimisation**

**Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment**

**Coactive Learning for Large Language Models using Implicit User Feedback**

**Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from Human Input**

**Active Preference Learning for Large Language Models**

**CONTRASTIVE PREFERENCE LEARNING: LEARNING FROM HUMAN FEEDBACK WITHOUT RL** ICLR2024 [paper](https://openreview.net/pdf?id=iX1RjVQODj)

**Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven Optimization**

**A State Augmentation based approach to Reinforcement Learning from Human Preferences**

**Dueling RL: Reinforcement Learning with Trajectory Preferences**

**Balancing Individual Preferences and Shared Objectives in Multiagent Reinforcement Learning**

**LaMP: When Large Language Models Meet Personalization**

**Aligning LLM Agents by Learning Latent Preference from User Edits**
