## RLHF Survey

As the reasoning ability of the LLM continues to improve, it is becoming increasingly important to align the LLM with human values. Therefore, **Christiano et al.2017 [1]** introduced the method to optimize LLMs using human feedback signals. And the method called reinforcement Learning based on human feedback(RLHF) was developed in Instruct-GPT **(Ouyang et al.2022 [2]; Bai et al.2022 [3])** Unlike traditional SFT methods, which estimate the next word generated by LLMs, RLHF learns human preferences and estimates the complete sentences generated by LLMs.

 In the traditional RLHF framework, human preferences are used to train a reward model $R$ that predicts the strength of human preferences for a given pair of outputs. The RL algorithm is then used to optimize the language model to maximize the rewards of this prediction**(Ouyang et al.2022 [4])**. A classic approach is to construct a reward model based on the MLE of the Bradley-Terry model**(Bradley et al.1957 [5])** and then using the PPO algorithm to optimize the reward signals with KL regularization.

Recently, relevant studies have analyzed that the off-line RLHF algorithm will bring certain defects. Due to the mode-seeking feature of inverse KL divergence itself, although RLHF is excellent in improving alignment performance, this feature also tends to reduce diversity in the generation process. In other words, the offline RLHF algorithm needs the offline data set to have a good coverage of the whole space, so as to ensure that the reward model can learn the optimal strategy**(Dong et al.2024 [6])**. So, **Xiong et al.2023 [7]** proposed an online iterative RLHF algorithm, collected some data by using the strategies generated during training, and added it to the training set of reward function to retrain the reward function.**.** **Zhu et al.2024 [8]** considered the iteration of data in the training process, and updated data labels in real time during training, so as to reduce the reward overfitting and over-optimization in RLHF In addition, there were studies that combined RLHF with the diversity framework QD to better reflect the human understanding of diversity**(Ding et al.2023 [9])**.

 In addition, the inevitable problem of RLHF is the complexity and instability of reward model fitting**(Choshen et al.2019[10])** , which may lead to increased computational complexity**(Yuan et al.2023 [11])**.

 Recent research has explored ways to directly optimize LLM strategies based on human preferences without relying on a scalar reward signal, The most classic improvement algorithm is the DPO algorithm **(Rafailov et al.2023 [12])** , the algorithm described the transformation of the RLHF problem through mathematical reasoning as a preference optimization problem rather than a reward estimation and maximization problem. Simplifying the alignment processes, reducing computational overhead, and achieving more robust optimizations by using preference data more directly.

 In the subsequent research, various work on further improvement based on DPO appeared.

**Zeng et al.2024 [13]** proposed that although DPO controls KL divergence from a sentence-level perspective, the model generation process is essentially token-by-token. It is intuitively indicated that DPO has limitations in fine-grained control and is weak in regulating KL divergence. Therefore, they propose TDPO approach, which allows further consideration of sequence effects without compromising alignment effects. 

 **Xu et al.2024 [14]** analyzed that the essence of DPO learning is to generate results preferred by humans as much as possible, while reducing the possibility of generating results not preferred by humans. This is actually a kind of contrast learning. Based on this, they further reduced the computational complexity by simplifying the loss function. 

 **Muldrew et al.2024 [15]** integrated the active learning strategy into the DPO method and selected more valuable and representative data to train the reward function. In addition, **Tang et al.2024 [16]** gave a unified view of offline preference optimization algorithms, arguing that the above mainstream algorithms can be abstracted into supervised binary classification problems.

### references

[1] Christiano, Paul Francis et al. “Deep Reinforcement Learning from Human Preferences.” *ArXiv* abs/1706.03741 (2017): n. pag.

[2] Ouyang, Long et al. “Training language models to follow instructions with human feedback.” *ArXiv* abs/2203.02155 (2022): n. pag.

[3] Bai, Yuntao et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” *ArXiv* abs/2204.05862 (2022): n. pag.

[4] Ouyang, Long et al. “Training language models to follow instructions with human feedback.” *ArXiv* abs/2203.02155 (2022): n. pag.

[5] Bradley R A, Terry ME. Rank analysis of incompiete block designs. I. Themethod of paired comparisons, 1957, 39:324-345.

[6] Dong, Hanze et al. “RLHF Workflow: From Reward Modeling to Online RLHF.” *ArXiv* abs/2405.07863 (2024): n. pag.

[7] Xiong, Wei et al. “Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint.” (2023).

[8] Zhu, Banghua et al. “Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF.” *ArXiv* abs/2401.16335 (2024): n. pag.

[9] Ding, Lijie et al. “Quality Diversity through Human Feedback.” *ArXiv* abs/2310.12103 (2023): n. pag.

[10] Choshen, Leshem et al. “On the Weaknesses of Reinforcement Learning for Neural Machine Translation.” *ArXiv* abs/1907.01752 (2019): n. pag.

[11] Yuan, Zheng et al. “RRHF: Rank Responses to Align Language Models with Human Feedback without tears.” *ArXiv* abs/2304.05302 (2023): n. pag.

[12] Rafailov, Rafael et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” *ArXiv* abs/2305.18290 (2023): n. pag.

[13] Zeng, Yongcheng et al. “Token-level Direct Preference Optimization.” *ArXiv* abs/2404.11999 (2024): n. pag.

[14] Xu, Haoran et al. “Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation.” *ArXiv* abs/2401.08417 (2024): n. pag.

[15] Muldrew, William et al. “Active Preference Learning for Large Language Models.” *ArXiv* abs/2402.08114 (2024): n. pag.

[16] Tang, Yunhao et al. “Generalized Preference Optimization: A Unified Approach to Offline Alignment.” *ArXiv* abs/2402.05749 (2024): n. pag.

